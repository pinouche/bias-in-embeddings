{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f10836f0-abbd-4602-a3ce-436577872e1c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd15b5f5-9c4f-46e0-9730-b1c41127eda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "from transformers import BitsAndBytesConfig, AutoModel\n",
    "import torch\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import pickle\n",
    "import requests\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b9ce5-b6cc-4912-b336-6c7b8948e8a6",
   "metadata": {},
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9498236f-30d4-4a10-8492-a4714903991f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    dict_characteristics = pickle.load(open(\"dict_characteristics.p\", \"rb\"))\n",
    "    dict_concepts = pickle.load(open(\"dict_concepts_bias.p\", \"rb\"))\n",
    "    return dict_characteristics, dict_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02d0cc7-477f-48a2-8fb1-e296768a8b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_string: str):\n",
    "    \n",
    "    model = None\n",
    "    if model_string in [\"Alibaba-NLP/gte-Qwen2-1.5B-instruct\",\n",
    "                        \"Alibaba-NLP/gte-large-en-v1.5\",\n",
    "                        \"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "                        \"nomic-ai/nomic-embed-text-v1\"]:\n",
    "        \n",
    "        model = SentenceTransformer(model_string, trust_remote_code=True, device=\"cpu\")\n",
    "\n",
    "    elif model_string in [\"dunzhang/stella_en_400M_v5\"]:\n",
    "        model = SentenceTransformer(model_string, trust_remote_code=True, device=\"cuda\")\n",
    "        \n",
    "    elif model_string not in [\"mistral:latest\", \n",
    "                              \"phi3:latest\", \n",
    "                              \"gemma2:latest\", \n",
    "                              \"qwen2:latest\",\n",
    "                              \"llama3.1:latest\",\n",
    "                              \"dunzhang/stella-en-1.5B-v5\", \n",
    "                              \"openai_model\"]:\n",
    "        \n",
    "        model = SentenceTransformer(model_string, device=\"cpu\")\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d02b69-42ae-43a0-b81c-191f6d680a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(model, text_input, model_string):\n",
    "\n",
    "    if isinstance(text_input, str):\n",
    "        text_input = [text_input]\n",
    "\n",
    "    if model_string in [\"mistral:latest\", \"phi3:latest\", \"gemma2:latest\", \"qwen2:latest\", \"llama3.1:latest\"]:\n",
    "\n",
    "        url = 'http://localhost:11434/api/embeddings'\n",
    "\n",
    "        embeddings = []\n",
    "        for text in text_input:\n",
    "            data = {\n",
    "                \"model\": model_string,\n",
    "                \"prompt\": text\n",
    "            }\n",
    "            \n",
    "            response = requests.post(url, json=data)\n",
    "            embedding = np.array(response.json()['embedding'])\n",
    "            embeddings.append(embedding)\n",
    "        embeddings = np.array(embeddings)\n",
    "\n",
    "    elif model_string in [\"dunzhang/stella-en-1.5B-v5\"]:\n",
    "\n",
    "        # here, we use prot-forwarding for reaching A100 on K8S\n",
    "        if model_string == \"dunzhang/stella-en-1.5B-v5\":\n",
    "            url = 'http://localhost:8001/v1/embeddings'\n",
    "\n",
    "        def query(payload, model_string):\n",
    "            if model_string == \"dunzhang/stella-en-1.5B-v5\":\n",
    "                payload[\"prompt_name\"] = \"s2s_query\"\n",
    "                payload[\"input\"] = payload[\"inputs\"]\n",
    "                del payload[\"inputs\"]\n",
    "                                         \n",
    "            response = requests.post(url, json=payload)\n",
    "            \n",
    "            if model_string == \"dunzhang/stella-en-1.5B-v5\":\n",
    "                embeddings = np.array([val[\"embedding\"] for val in response.json()[\"data\"]])\n",
    "            else:\n",
    "                embeddings = np.array(response.json())\n",
    "            # embeddings = np.array([element[\"embedding\"] for element in data])\n",
    "\n",
    "            return embeddings\n",
    "\n",
    "        max_chunk_size = 16\n",
    "        if len(text_input) > max_chunk_size:\n",
    "            num_chunks = (len(text_input) // max_chunk_size) + 1\n",
    "        embeddings_list = []\n",
    "        for i in range(0, len(text_input), max_chunk_size):\n",
    "            chunk = text_input[i:i + max_chunk_size]\n",
    "            \n",
    "            embedding = query({\n",
    "                    \t\"inputs\": list(chunk),\n",
    "                        #\"normalize\": True,\n",
    "                        \"prompt_name\": \"query\"\n",
    "                    }, model_string)\n",
    "\n",
    "            embeddings_list.append(embedding)\n",
    "        embeddings = np.concatenate(embeddings_list)\n",
    "                \n",
    "    else:\n",
    "        embeddings = model.encode(list(text_input), normalize_embeddings=True)\n",
    "        \n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae253734-dba8-44fa-abb9-4b08db2207c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_neutral(dic_pairs, model, model_string):\n",
    "    \"\"\"This embbeds the concepts counterparts\"\"\"\n",
    "\n",
    "    first_concept_words = np.array(list(dic_pairs.keys()))\n",
    "    second_concept_words = np.array(list(dic_pairs.values()))\n",
    "    \n",
    "    first_concept_embeddings = get_embeddings(model, first_concept_words, model_string)\n",
    "    second_concept_embeddings = get_embeddings(model, second_concept_words, model_string)\n",
    "\n",
    "    embeddings_neutral = first_concept_embeddings - second_concept_embeddings\n",
    "\n",
    "    return embeddings_neutral, first_concept_embeddings, second_concept_embeddings, first_concept_words, second_concept_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de4c881-3803-423b-8df4-621ccec6af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"dunzhang/stella-en-1.5B-v5\",\n",
    "              \"Alibaba-NLP/gte-large-en-v1.5\", \n",
    "              \"mixedbread-ai/mxbai-embed-large-v1\",\n",
    "              \"WhereIsAI/UAE-Large-V1\",\n",
    "              \"intfloat/multilingual-e5-large-instruct\",\n",
    "              \"avsolatorio/GIST-large-Embedding-v0\",\n",
    "              \"BAAI/bge-large-en-v1.5\",\n",
    "              \"llmrails/ember-v1\",\n",
    "              \"nomic-ai/nomic-embed-text-v1\",\n",
    "              \"sentence-transformers/all-mpnet-base-v2\",\n",
    "              \"sentence-transformers/sentence-t5-xl\",\n",
    "              \"sentence-transformers/all-MiniLM-L12-v2\",\n",
    "              \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "              \"FacebookAI/xlm-roberta-base\",\n",
    "              \"mistral:latest\",\n",
    "              \"llama3.1:latest\",\n",
    "              \"phi3:latest\",\n",
    "              \"gemma2:latest\",\n",
    "              \"qwen2:latest\",\n",
    "             ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbea48e-75b8-4649-99d0-b290a261ec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA projection\n",
    "\n",
    "def pca_on_neutral_embeddings(embeddings_neutral):\n",
    "\n",
    "    n = 10\n",
    "    m = min(embeddings_neutral.shape[0], embeddings_neutral.shape[1])\n",
    "    if m < n:\n",
    "        n = m\n",
    "        \n",
    "    pca_neutral = PCA(n_components=n)\n",
    "    scaler_neutral = StandardScaler()\n",
    "\n",
    "    scaler_neutral.fit(embeddings_neutral)\n",
    "    embeddings_neutral = scaler_neutral.transform(embeddings_neutral)\n",
    "\n",
    "    pca_neutral.fit(embeddings_neutral)\n",
    "    neutral_emb = pca_neutral.transform(embeddings_neutral)\n",
    "\n",
    "    return neutral_emb, pca_neutral, scaler_neutral"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1825dd-94b0-4775-b888-0c8bd6c70299",
   "metadata": {},
   "source": [
    "# which PC separates the data best and what is the concept direction \"strength\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0fa30-6958-42a7-acad-d41b405ced0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bias_hardness(array):\n",
    "    ''' returns a number between 0 and 1. 0 means very difficult bias to discern. 0.5 means very easy/strong apriori bias'''\n",
    "    perceived_bias_hardness = 2*np.mean(np.abs(array-0.5))\n",
    "    return perceived_bias_hardness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a79f1c6-0d9e-4535-a47d-8b08047289ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the quality of the concept direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022a9e0-1078-46bf-a265-ba7525865105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(x, y, texts, concept: str, attribute: str, model: str, concept_axis: int, auc: float):\n",
    "    model = model.replace(\"/\", \"-\")\n",
    "    file_path = f\"plots/pc_projection/{concept}_{attribute}/model_{model}.pdf\"\n",
    "\n",
    "    # Extract directory path\n",
    "    directory = os.path.dirname(file_path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    c = np.array([\"darkblue\"]*len(y), dtype='<U16')\n",
    "    c[y == 1] = \"darkorange\"\n",
    "\n",
    "    i = concept_axis\n",
    "    if i == 0:\n",
    "        j = 1\n",
    "    else:\n",
    "        j = (concept_axis-1)\n",
    "\n",
    "    plt.plot()\n",
    "    plt.grid(True)\n",
    "    plt.scatter(x[:, i], x[:, j], c=c, marker=\"+\", s=40)  # Using mod to loop over PCs\n",
    "    for index, label in enumerate(texts):\n",
    "        plt.text(x[index, i], x[index, j]+(np.max(x[:, j])-np.min(x[:, j]))/40, label, ha='center', va='bottom')\n",
    "    plt.title(f\"The AUC for PC {i} is {np.round(auc, 2)}.\")\n",
    "    plt.xlabel(f'PC {i}')\n",
    "    plt.ylabel(f'PC {j}')\n",
    "    \n",
    "    plt.tight_layout()  # Adjust layout to prevent overlapping\n",
    "    plt.savefig(file_path, format='pdf', dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c30897-e933-424b-916d-4f2e023e7e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concept_strenght(first_concept_embeddings, second_concept_embeddings, scaler, pca):\n",
    "    x = np.vstack((first_concept_embeddings, second_concept_embeddings))\n",
    "    #x = scaler.transform(x)\n",
    "    x = pca.transform(x)\n",
    "    y = np.array([0]*first_concept_embeddings.shape[0]+[1]*first_concept_embeddings.shape[0])\n",
    "    \n",
    "    correlation_list = []\n",
    "    auc_list = []\n",
    "    for i in range(x.shape[1]):\n",
    "        pc_data = x[:, i]\n",
    "\n",
    "        best_auc = 0\n",
    "        for label_position in [0, 1]:\n",
    "            fpr, tpr, thresholds = roc_curve(y, pc_data, pos_label=label_position)\n",
    "            metric_auc = auc(fpr, tpr)\n",
    "            if metric_auc > best_auc:\n",
    "                best_auc = metric_auc\n",
    "        auc_list.append(best_auc)\n",
    "\n",
    "    # this correlation list is to decide which PC is most important\n",
    "    concept_axis = np.argmax(np.abs(auc_list))\n",
    "    print(\"AUC LIST\", auc_list)\n",
    "    # this would be the concept direction strength.\n",
    "    concept_strength = np.max(np.abs(auc_list))\n",
    "\n",
    "    # check which pair is farthest away (most representative)\n",
    "    diff = np.abs(x[:int(x.shape[0]/2), concept_axis] - x[int(x.shape[0]/2):, concept_axis])\n",
    "    argmax = np.argmax(diff)\n",
    "\n",
    "    return x, y, concept_axis, concept_strength, argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e07ca9b-232c-453c-ac4d-ed9dd3514155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_concept_strength(dic_pairs_concept, model, concept, attribute, model_string, plot=False):\n",
    "\n",
    "    embeddings_neutral, first_concept_embeddings, second_concept_embeddings, first_concept_words, second_concept_words = embed_neutral(dic_pairs_concept,\n",
    "                                                                                                                                       model,\n",
    "                                                                                                                                       model_string)\n",
    "    neutral_emb, pca_neutral, scaler_neutral = pca_on_neutral_embeddings(embeddings_neutral)\n",
    "\n",
    "    x, y, concept_axis, concept_strength, argmax = compute_concept_strenght(first_concept_embeddings, \n",
    "                                                                            second_concept_embeddings, \n",
    "                                                                            scaler_neutral, \n",
    "                                                                            pca_neutral)    \n",
    "\n",
    "    texts = np.hstack((list(dic_pairs_concept.keys()), list(dic_pairs_concept.values())))\n",
    "    \n",
    "    print(\"Most discriminative pair\", \n",
    "          np.array(list(dic_pairs_concept.keys()))[argmax],\n",
    "          np.array(list(dic_pairs_concept.values()))[argmax])\n",
    "\n",
    "    discriminative_pair = (np.array(list(dic_pairs_concept.keys()))[argmax], np.array(list(dic_pairs_concept.values()))[argmax])\n",
    "    \n",
    "    if plot:\n",
    "        plot_scatter(x, y, texts, concept, attribute, model_string, concept_axis, concept_strength)\n",
    "    \n",
    "    return concept_strength, concept_axis, pca_neutral, scaler_neutral, discriminative_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7062bf22-63a1-4af5-92a0-5030e035d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_projections(x, num_projections=100):\n",
    "\n",
    "    n_features = x.shape[1]\n",
    "    random_vectors = np.random.normal(size=(n_features, num_projections))\n",
    "    normed_vectors = random_vectors / np.linalg.norm(random_vectors, axis=1, keepdims=True)\n",
    "\n",
    "    projections = np.matmul(x, normed_vectors)\n",
    "    \n",
    "    return projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b99dd3-1b17-4dce-8767-4ce5058a4b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_concept_bias(characteristics, concept_index, pca_model, scaler_model, model, model_string, \n",
    "                          concept: str, attribute: str, original_attributes: list, context: tuple, plot=False):\n",
    "\n",
    "    texts = list(characteristics.keys())\n",
    "    labels = np.array(list(characteristics.values()))\n",
    "\n",
    "    # here, add bias strength for humans: \"perceived bias prevalence\"\n",
    "    perceived_bias_prevalence = compute_bias_hardness(labels)\n",
    "    print(f\"perceived_bias_prevalence: {perceived_bias_prevalence}.\")\n",
    "\n",
    "    characteristics_embeddings = get_embeddings(model, np.array(texts), model_string)\n",
    "    # here, we want to add random projections correlation list\n",
    "    projections = random_projections(characteristics_embeddings, 10000)\n",
    "\n",
    "    corr_list = []\n",
    "    for i in range(projections.shape[1]):\n",
    "        corr = np.corrcoef(labels, projections[:, i])[0,1]\n",
    "        corr_list.append(corr)\n",
    "    \n",
    "    characteristics_embeddings = scaler_model.transform(characteristics_embeddings)\n",
    "    characteristics_embeddings = pca_model.transform(characteristics_embeddings)\n",
    "\n",
    "    characteristics_projection = characteristics_embeddings[:, concept_index]\n",
    "    bias_strenght = np.corrcoef(characteristics_projection, labels)[0, 1]\n",
    "\n",
    "    p_value = np.sum(np.abs(bias_strenght) < np.abs(corr_list))/len(corr_list)\n",
    "\n",
    "    if plot:\n",
    "        model_string = model_string.replace(\"/\", \"-\")\n",
    "        \n",
    "        # plot the correlation scatter plot\n",
    "        file_path = f\"plots/correlation_plots/{concept}_{attribute}/{str(add_context)}/model_{model_string}.pdf\"\n",
    "        # Extract directory path\n",
    "        directory = os.path.dirname(file_path)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        plt.plot()\n",
    "        plt.grid()\n",
    "        plt.ylabel(\"Human-annotated labels\")\n",
    "        plt.xlabel(\"Projection onto the concept direction\")\n",
    "        plt.scatter(characteristics_projection, labels, c=\"darkblue\", marker=\"+\", s=40)\n",
    "        for index, label in enumerate(original_attributes):\n",
    "            plt.text(characteristics_projection[index], labels[index]+(np.max(labels)-np.min(labels))/40, label, \n",
    "                     ha='center', va='bottom', fontsize=8)\n",
    "        # plt.plot([min(characteristics_projection), max(characteristics_projection)], \n",
    "        #          [min(labels), max(labels)], color='red', linestyle='--', label='y=x')\n",
    "        plt.title(f\"$\\\\rho$={np.round(bias_strenght, 2)}\")\n",
    "        plt.savefig(file_path, format='pdf', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "        # plot the histograms\n",
    "        file_path = f\"plots/histograms/{concept}_{attribute}/{str(add_context)}/model_{model_string}.pdf\"\n",
    "\n",
    "        # Extract directory path\n",
    "        directory = os.path.dirname(file_path)\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        \n",
    "        plt.plot()\n",
    "        plt.grid()\n",
    "        plt.hist(corr_list, bins=50)\n",
    "        plt.axvline(np.median(corr_list), color='darkblue', linestyle='--', linewidth=2)\n",
    "        plt.axvline(bias_strenght, color='darkred', linestyle='--', linewidth=2)\n",
    "        plt.title(f\"p-value={np.round(p_value, 2)}\")\n",
    "        plt.legend([\"median\", \"bias correlation\"])\n",
    "        plt.savefig(file_path, format='pdf', dpi=300)\n",
    "        plt.show()\n",
    "    \n",
    "        # plot_scatter(characteristics_embeddings, labels, texts)\n",
    "    \n",
    "    return bias_strenght, p_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb4aa84-93af-48e0-b756-773424735bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_context_to_attributes(concept, key, bias_context):\n",
    "\n",
    "    if bias_context == \"debiasing\":\n",
    "        if concept == \"gender\":\n",
    "            new_key = f'''I want to find information about a person. This person's profession is {key}. This person's gender is unknown.''' \n",
    "        elif concept == \"age\":\n",
    "            new_key = f'''I want to find information about a person. This person's character trait is {key}. This person's age is unknown.''' \n",
    "        elif concept == 'poor-rich':\n",
    "            new_key = f'''I want to find information about a person. This person is of {key} ethnicity. This person's social status is unknown.'''\n",
    "    \n",
    "    elif bias_context == \"positive\":\n",
    "        if concept == \"gender\":\n",
    "            new_key = f'''I want to find information about a person. This person's profession is {key}. This person is a woman.''' \n",
    "        elif concept == \"age\":\n",
    "            new_key = f'''I want to find information about a person. This person's character trait is {key}. This person is old.'''\n",
    "        elif concept == 'poor-rich':\n",
    "            new_key = f'''I want to find information about a person. This person is of {key} ethnicity. This person is poor.'''\n",
    "    \n",
    "    elif bias_context == \"negative\":\n",
    "        if concept == \"gender\":\n",
    "            new_key = f'''I want to find information about a person. This person's profession is {key}. This person is a man.''' \n",
    "        elif concept == \"age\":\n",
    "            new_key = f'''I want to find information about a person. This person's character trait is {key}. This person is young.''' \n",
    "        elif concept == 'poor-rich':\n",
    "            new_key = f'''I want to find information about a person. This person is of {key} ethnicity. This person is wealthy.'''\n",
    "    \n",
    "    elif bias_context is None:\n",
    "        if concept == \"gender\":\n",
    "            new_key = f'''I want to find information about a person. This person's profession is {key}.'''\n",
    "        elif concept == \"age\":\n",
    "            new_key = f'''I want to find information about a person. This person is {key}.'''\n",
    "        elif concept == 'poor-rich':\n",
    "            #new_key = key\n",
    "            new_key = f'''I want to find information about a person. This person is of {key} ethnicity.'''\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Not a valid bias_context {bias_context}.\")\n",
    "    \n",
    "    return new_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338fc6b6-b381-4893-b585-be24edf8a5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_concept_strenght_vs_bias_strenght(score_dict: dict, a: str, c: str):\n",
    "#     plt.figure(figsize=(10, 10))\n",
    "#     plt.grid()\n",
    "    \n",
    "#     concept_strenght = [tup[1] for tup in list(score_dict.values())[0]]\n",
    "#     plt.scatter(np.arange(len(concept_strenght)), \n",
    "#                 np.array(concept_strenght),\n",
    "#                 marker=\"v\", edgecolors=\"darkgreen\", facecolors=\"none\", s=50)\n",
    "\n",
    "#     colors_list = [\"darkred\", \"darkblue\", \"darkorange\"]\n",
    "#     markers_list = [\"o\", \"s\", \"d\"]\n",
    "    \n",
    "#     counter = 0  # Start from 1 since 0 is used for concept strength\n",
    "#     for k, v in score_dict.items():\n",
    "        \n",
    "#         plt.scatter(np.arange(len(concept_strenght)), \n",
    "#                     np.array([tup[0] for tup in v]),\n",
    "#                     marker=markers_list[counter], \n",
    "#                     edgecolors=colors_list[counter], facecolors=\"none\", s=50)\n",
    "        \n",
    "#         counter += 1\n",
    "    \n",
    "#     plt.xticks(np.arange(len(concept_strenght)), model_list, rotation=90)\n",
    "#     plt.ylabel(\"p-value & AUC\")\n",
    "#     plt.title(f\"Concept={a}, Attribute={c}\")\n",
    "\n",
    "#     # Adjust legend placement to be above the plot\n",
    "#     plt.legend([\"Concept strength (AUC)\",\n",
    "#                 \"RAG\",\n",
    "#                 \"RAG + debiasing\",\n",
    "#                 \"RAG + positive\"], \n",
    "#                loc='lower center', bbox_to_anchor=(0.5, 1.05), ncol=4)\n",
    "    \n",
    "#     plt.tight_layout()  # Adjust layout to fit legend above plot\n",
    "#     plt.savefig(f\"{a}_{c}_finalplot.pdf\", format='pdf', dpi=300)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e625b7dd-34da-4c22-b611-5de7093d9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_characteristics, dict_concepts_bias = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae1c59a-09c9-427e-875d-3c4722e00063",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dict_characteristics.keys(), dict_concepts_bias.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53142795-9bba-4d33-90c7-bb9ea4c07478",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_concept_attributes = [('gender', 'occupations'),\n",
    "                            ('age', 'age-characteristics')\n",
    "                            ('poor-rich', 'ethnicities_scores'),\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531cf01-a3c3-414f-b03b-e61ddc4006c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639878be-4cc5-4cf1-b31d-a31afdfddbdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in pairs_concept_attributes:\n",
    "    c, a = tup\n",
    "    score_dict = {}\n",
    "\n",
    "    # First boolean is for the context of the RAG query, second boolean is for the bias-related context\n",
    "    for add_context in [(True, None), (True, \"debiasing\"), (True, \"positive\"), (True, \"negative\")]:\n",
    "        rag_context = add_context[0]\n",
    "        bias_context = add_context[1]\n",
    "        parameter_string = f\"rag={rag_context}_neutral_context={bias_context}\"\n",
    "        \n",
    "        if parameter_string not in score_dict:\n",
    "            score_dict[parameter_string] = []\n",
    "            \n",
    "        for model_string in model_list:\n",
    "            model = get_model(model_string)\n",
    "            print(f\"Computing for model {model_string} and for {parameter_string}.\")\n",
    "            # for attributes in dict_characteristics.keys():\n",
    "            characteristics = dict_characteristics[a]\n",
    "            concept_pairs = dict_concepts_bias[c]\n",
    "\n",
    "            attributes = list(characteristics.keys())\n",
    "            values = list(characteristics.values())\n",
    "\n",
    "            if rag_context:\n",
    "                new_attributes = []\n",
    "                for attribute in attributes:\n",
    "                    attribute = add_context_to_attributes(c, attribute, bias_context)\n",
    "                    new_attributes.append(attribute)\n",
    "            else:\n",
    "                new_attributes = attributes\n",
    "\n",
    "            characteristics = dict(zip(new_attributes, characteristics.values()))\n",
    "                            \n",
    "            print(f\"---------Computing for concept '{c}' and attributes '{a}'.----------\")\n",
    "            concept_strength, concept_index, pca_neutral, scaler_neutral, discriminative_pair = pipeline_concept_strength(concept_pairs, model, a, c, \n",
    "                                                                                                                          model_string, True)\n",
    "            print(f\"The concept {c} strength (auc between predictions and true labels for the concept terms) is: {concept_strength}\")\n",
    "                            \n",
    "            bias_strength, p_value = pipeline_concept_bias(characteristics, concept_index, pca_neutral, scaler_neutral, model, \n",
    "                                                           model_string, a, c, attributes, add_context, True)\n",
    "            print(f\"The bias strength (correlation between predictions and true labels for the biased terms) is: {bias_strength}\")\n",
    "            print(f\"The p_value is: {p_value}\")\n",
    "                    \n",
    "            score_dict[parameter_string].append((p_value, concept_strength, discriminative_pair, bias_strength))\n",
    "        \n",
    "            del model \n",
    "            gc.collect()\n",
    "\n",
    "    path = f\"results/method_1/score_dict_{c}_{a}.p\"\n",
    "    directory = os.path.dirname(path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    pickle.dump(score_dict, open(path, \"wb\"))\n",
    "    # plot_concept_strenght_vs_bias_strenght(score_dict, c, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cedb2e-ecd4-43b1-9e18-800bcec94ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1827a-912c-4bf8-a04e-8ad5116c7bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e344adc4-40c4-48c3-a900-5946007d8b79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b185b22b-d11c-4ffd-b338-fe50fed6e62f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df10b56d-327e-4af2-9a0c-91069a9edc9a",
   "metadata": {},
   "source": [
    "# WEAT tests (directly using cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45037994-2b1f-45b8-bd51-2636de2ee27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon, ttest_rel, binomtest\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c894cd-4ab4-498d-963a-f2e388eae2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantics derived automatically from language corpora contain human-like biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276c9e9f-baba-4719-9c79-ab2fc8357a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_dims(array):\n",
    "    if len(array.shape) == 1:\n",
    "        array = np.expand_dims(array, 0)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771f0c5f-f9e5-4bf3-b613-4aa8a85fac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_dict(model, model_string: str, words_list: list[str]):\n",
    "\n",
    "    dict_embeddings = {}\n",
    "    embeddings = get_embeddings(model, words_list, model_string)\n",
    "    for i, word in enumerate(words_list):\n",
    "        dict_embeddings[word] = embeddings[i, :]\n",
    "\n",
    "    return dict_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cac50e2-1e96-4435-8687-c5c077a47a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_list(concept, attributes, values, rag_context, bias_context):\n",
    "\n",
    "    if rag_context:\n",
    "        attributes = add_context_to_attributes(concept, attributes, bias_context)\n",
    "    \n",
    "    target_set_one = list(dict_concepts_bias[concept].keys())\n",
    "    target_set_two = list(dict_concepts_bias[concept].values())\n",
    "    \n",
    "    words_list = attributes + target_set_one + target_set_two\n",
    "\n",
    "    return words_list, attributes, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d64828-18d6-479f-9d0f-18292da30350",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_characteristics, dict_concepts_bias = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6f303e-acbd-4036-a65c-1a4ff296ae1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de3ba13-b235-450e-910c-8797e6d10af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13282b7-e09d-486b-8a5d-95bed85939b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65721d-2efc-4353-80cf-00a6328f5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_concept_attributes = [('gender', 'occupations'),\n",
    "                            ('age', 'age-characteristics')\n",
    "                            ('poor-rich', 'ethnicities_scores'),\n",
    "                           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11300f6-73ee-4b8f-825e-15b7144ce4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a5418c-b65e-4582-ac6e-d3e67793d33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tup in pairs_concept_attributes:\n",
    "    print(f\"Computing for {tup}.\")\n",
    "    \n",
    "    concept, attribute_key = tup\n",
    "    score_dict = {}\n",
    "\n",
    "    for add_context in [(True, None), (True, \"debiasing\"), (True, \"positive\"), (True, \"negative\")]:\n",
    "        rag_context = add_context[0]\n",
    "        bias_context = add_context[1]\n",
    "        parameter_string = f\"rag={rag_context}_neutral_context={bias_context}\"\n",
    "        print(f\"Computing for parameters: {parameter_string}.\")\n",
    "        \n",
    "        if parameter_string not in score_dict:\n",
    "            score_dict[parameter_string] = []\n",
    "\n",
    "        for model_string in model_list:\n",
    "            print(f\"Computing for model: {model_string}.\")\n",
    "        \n",
    "            list_average_diff = []\n",
    "        \n",
    "            attributes = list(dict_characteristics[attribute_key].keys())\n",
    "            values = list(dict_characteristics[attribute_key].values())\n",
    "\n",
    "            if rag_context:\n",
    "                new_attributes = []\n",
    "                for attribute in attributes:\n",
    "                    attribute = add_context_to_attributes(concept, attribute, bias_context)\n",
    "                    new_attributes.append(attribute)\n",
    "            else:\n",
    "                new_attributes = attributes\n",
    "        \n",
    "            model = get_model(model_string)\n",
    "            # embed attributes\n",
    "            e_attributes = get_embeddings(model, new_attributes, model_string)\n",
    "        \n",
    "            # embed targets\n",
    "            dict_embedding_targets = {}\n",
    "            for target_1, target_2 in dict_concepts_bias[concept].items():\n",
    "                e_target_1 = get_embeddings(model, target_1, model_string)\n",
    "                e_target_2 = get_embeddings(model, target_2, model_string)\n",
    "                dict_embedding_targets[target_1] = e_target_1\n",
    "                dict_embedding_targets[target_2] = e_target_2\n",
    "    \n",
    "            list_diff_all_context_average = []\n",
    "            list_diff_neutral_context_average = []\n",
    "            for i, e_att in enumerate(e_attributes):\n",
    "                # print(f\"Computing for attribute {new_attributes[i]}\")\n",
    "                list_diff = []\n",
    "                list_diff_neutral = []\n",
    "                \n",
    "                v = values[i]\n",
    "                \n",
    "                for target_1, target_2 in dict_concepts_bias[concept].items():\n",
    "                    \n",
    "                    e_target_1 = dict_embedding_targets[target_1]\n",
    "                    e_target_2 = dict_embedding_targets[target_2]\n",
    "                    \n",
    "                    sim_target_1 = cosine_similarity(e_target_1, np.expand_dims(e_att, 0))[0]\n",
    "                    sim_target_2 = cosine_similarity(e_target_2, np.expand_dims(e_att, 0))[0]\n",
    "\n",
    "                    # print(f\"cos sim: {target_1}, {new_attributes[i]}\", sim_target_1)\n",
    "                    # print(f\"cos sim: {target_2}, {new_attributes[i]}\", sim_target_2)\n",
    "\n",
    "                    # For neutral, we want to test further what is the effect and test it the same way as \"positive\"\n",
    "                    alternative = \"greater\"\n",
    "                    p = 0.5\n",
    "                    if bias_context in [None, \"debiasing\"]:\n",
    "                        diff = sim_target_2 - sim_target_1\n",
    "                        if v < 0.5:\n",
    "                            diff = sim_target_1 - sim_target_2\n",
    "\n",
    "                        # here, we check if the \"discriminated\" against group is always bigger. Same as \"positive\" case.\n",
    "                        if bias_context == \"debiasing\":\n",
    "                            alternative = \"two-sided\"\n",
    "                            diff_neutral = sim_target_1 - sim_target_2\n",
    "                            list_diff_neutral.append(diff_neutral)\n",
    "        \n",
    "                    elif bias_context == \"positive\":\n",
    "                        p = np.mean(np.array(values) < 0.5)\n",
    "                        diff = sim_target_1 - sim_target_2\n",
    "                    elif bias_context == \"negative\":\n",
    "                        p = np.mean(np.array(values) < 0.5)\n",
    "                        alternative = \"less\"\n",
    "                        diff = sim_target_1 - sim_target_2\n",
    "                    \n",
    "                    list_diff.append(diff)\n",
    "                # print(f\"The mean is {np.sum([val > 0 for val in list_diff])}\")\n",
    "                list_diff_all_context_average.append(int(np.mean(list_diff) > 0))\n",
    "\n",
    "                if bias_context == \"debiasing\":\n",
    "                    list_diff_neutral_context_average.append(int(np.mean(list_diff_neutral) > 0))\n",
    "\n",
    "            ##########\n",
    "            p_value = binomtest(np.sum(list_diff_all_context_average), len(list_diff_all_context_average), p=p, alternative=alternative)\n",
    "            print(p_value)\n",
    "            \n",
    "            # if we see that we do not reject H_0 at alpha=5% level, we do a second test to investigate what the effect is of adding \n",
    "            # debiasing/neutral context. In particular, we test if it is skewing the results the opposite way: everything is now closer to \n",
    "            # female terms and old terms.\n",
    "            p_value_neutral = None\n",
    "            if bias_context == \"debiasing\" and p_value.pvalue > 0.05:\n",
    "                hypothesis_p = np.mean(np.array(values) < 0.5)\n",
    "                p_value_neutral = binomtest(np.sum(list_diff_neutral_context_average), \n",
    "                                            len(list_diff_neutral_context_average), \n",
    "                                            p=hypothesis_p, \n",
    "                                            alternative=\"greater\")\n",
    "                print(\"check if opposite skew\")\n",
    "                print(p_value_neutral)\n",
    "\n",
    "            score_dict[parameter_string].append((p_value, p_value_neutral))\n",
    "\n",
    "            del model \n",
    "            gc.collect()\n",
    "\n",
    "    path = f\"results/method_2/score_dict_{concept}_{attribute_key}.p\"\n",
    "    directory = os.path.dirname(path)\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "    pickle.dump(score_dict, open(path, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab1f503-b866-470a-b3de-ed8d8c95f2c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e772e3d-0a56-4675-8c5f-912bd17ea4f5",
   "metadata": {},
   "source": [
    "# Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af278d4c-05dc-46a5-8871-b846479341eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mteb_scores = [71.19, 65.39, 64.68, 64.64, 64.41, 64.34, 64.23, 63.34, 62.39, 57.87, 57.77, 56.46, 56.09, 56.09]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3b784-efc7-48dc-8042-2ba3d7b308fb",
   "metadata": {},
   "source": [
    "## For method 1 (geometrical bias detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f1a425-1d29-4818-b5f9-2eb23462d9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7713e096-a432-433d-b5d5-e9cc52b7bf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'results/method_1'\n",
    "\n",
    "data_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    print(filepath)\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6196ce0c-0716-4bba-bc79-e69ee514c20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559b5de-7cbc-48f1-a476-1f8bd9987d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corr = {}\n",
    "\n",
    "for dic in data_list:\n",
    "    correlation_list = []\n",
    "    for k, v in dic.items():\n",
    "        auc = np.array([np.abs(tup[1]) for tup in v])\n",
    "        correlations = np.array([np.abs(tup[-1]) for tup in v])\n",
    "\n",
    "        if k not in dict_corr:\n",
    "            dict_corr[k] = []\n",
    "        dict_corr[k].append(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e0f37-be44-4cfc-bb7b-9996d9a4fbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_gender = [0.96, 0.96, 0.93, 0.94, 0.97, 0.927, 0.87, 0.86, 0.84, 0.73, 0.90, 0.81, 0.94, 0.72, 0.82, 0.74, 0.65, 0.76, 0.75]\n",
    "auc_wealth = [0.93, 0.79, 0.90, 0.90, 0.83, 0.87, 0.88, 0.88, 0.71, 0.66, 0.91, 0.74, 0.67, 0.61, 0.71, 0.71, 0.71, 0.67, 0.68]\n",
    "auc_age = [0.90, 0.92, 0.89, 0.89, 0.90, 0.91, 0.86, 0.85, 0.82, 0.88, 0.79, 0.90, 0.86, 0.72, 0.63, 0.65, 0.69, 0.68, 0.66]\n",
    "average_auc = (np.array(auc_gender)+np.array(auc_age)+np.array(auc_wealth))/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f44a8-aa55-4788-98c7-8637a402ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#4682B4\", \"#2E8B57\", \"#DC143C\", \"#D2691E\"]\n",
    "values_list_method_one = []\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.grid()\n",
    "plt.plot(range(len(average_auc)), average_auc, \"--\", c=\"#DDA0DD\", marker='o', markersize=4)\n",
    "\n",
    "count = 0\n",
    "for k, v in dict_corr.items():\n",
    "    print(f\"Computing for key {k}.\")\n",
    "    values = np.mean(np.array(v), axis=0)\n",
    "    print(\"corr 1\", np.corrcoef(np.array(model_mteb_scores), values[:len(model_mteb_scores)])[0,1]) \n",
    "    plt.plot(range(len(values)), values, c=colors[count], marker='o', markersize=4, linewidth=1)\n",
    "    values_list_method_one.append(values)\n",
    "    count += 1\n",
    "\n",
    "print(\"corr 2\", np.corrcoef(np.array(model_mteb_scores), average_auc[:len(model_mteb_scores)])[0,1]) \n",
    "plt.ylabel(r\"$\\rho$ & AUC\", size=11.5)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xticks(range(len(values)), ['']*len(values))\n",
    "legend = plt.legend([\"AUC\", \"Neutral\", \"Debiasing\", \"Positive\", \"Negative\"], \n",
    "                    loc=\"upper right\",\n",
    "                    bbox_to_anchor=(1, 1.03))\n",
    "legend.get_frame().set_alpha(0.2) \n",
    "plt.savefig(f\"method1_finalplot.pdf\", format='pdf', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4d44f-d610-4c5b-a760-79926133b3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4032ba75-1289-4e76-84e8-58033532c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = np.array(values_list_method_one[0]-values_list_method_one[1])[:len(model_mteb_scores)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f3cfe-79b4-4219-a1a9-8aa567d1483b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.corrcoef(np.array(model_mteb_scores), diff)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea336aa-a1ec-40c5-b4ca-e80adbdd8475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81f49aaf-d80f-4a12-81d9-ad92be968857",
   "metadata": {},
   "source": [
    "## For method 2 (WEAT bias detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314962bb-082d-42d8-8497-93d7446c286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'results/method_2'\n",
    "\n",
    "data_list = []\n",
    "for filename in os.listdir(directory):\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    print(filepath)\n",
    "    if os.path.isfile(filepath):\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5999841-8072-4b30-bbc6-c2c7c5025035",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820656e8-dcf5-4107-be16-93b18d4ba8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corr = {}\n",
    "\n",
    "for dic in data_list:\n",
    "    correlation_list = []\n",
    "    for k, v in dic.items():\n",
    "        print(k)\n",
    "        pvalues = correlations = np.array([tup[0].pvalue if tup[1] is None else min(tup[0].pvalue, tup[1].pvalue) for tup in v])\n",
    "        correlations = np.array([tup[0].statistic if tup[1] is None else max(tup[0].statistic, tup[1].statistic) for tup in v])\n",
    "\n",
    "        print([[pvalues[i], correlations[i]] for i in range(len(correlations))])\n",
    "\n",
    "        if k not in dict_corr:\n",
    "            dict_corr[k] = []\n",
    "        dict_corr[k].append(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4799c01-083b-443a-b949-3650ddbf743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f8a29-44fb-40ec-a9ab-40b209d88d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab68b85-4052-497e-b1dd-cc818b07267a",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#4682B4\", \"#2E8B57\", \"#DC143C\", \"#D2691E\"]\n",
    "values_list_method_two = []\n",
    "\n",
    "plt.figure(figsize=(8, 6.3))\n",
    "plt.grid()\n",
    "\n",
    "count = 0\n",
    "for k, v in dict_corr.items():\n",
    "    values = np.mean(np.array(v), axis=0)\n",
    "    #print(values)\n",
    "    print(\"corr 1\", np.corrcoef(np.array(model_mteb_scores), values[:len(model_mteb_scores)])[0,1]) \n",
    "    plt.plot(range(len(values)), values, c=colors[count], marker='o', markersize=4, linewidth=1)\n",
    "    values_list_method_two.append(values)\n",
    "    count += 1\n",
    "\n",
    "plt.ylabel(r\"$\\hat{p}$\", size=12)\n",
    "plt.ylim(-0.1, 1.1)\n",
    "print(len(values))\n",
    "plt.xticks(range(len(values)), [model.split(\"/\")[1] if \"/\" in model else model for model in model_list], rotation=90, size=12)\n",
    "legend = plt.legend([\"Neutral\", \"Debiasing\", \"Positive\", \"Negative\"], \n",
    "                    loc=\"upper left\",\n",
    "                    bbox_to_anchor=(0, 0.44),\n",
    "                   fontsize=11)\n",
    "legend.get_frame().set_alpha(0.2) \n",
    "plt.tick_params(axis='y', labelsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"method2_finalplot.pdf\", format='pdf', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2509209-64b0-4cfc-9662-922216085f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list_method_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d28fa4-2887-4c3b-b356-4410110ffde7",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_list_method_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600ab874-c6f4-4d1f-b5e1-63b60bfdef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761de3fb-ffa5-4781-a976-2b5bc611847f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086d1c8-e5c5-4aed-8470-151287e04d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e540391d-b8f5-419b-8825-8ceb49872d73",
   "metadata": {},
   "source": [
    "# Retrieval example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf194f4-5e10-423e-a02a-eefedaaedb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f2b942-be3d-49ab-b2ee-7fc97e02a621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarities(query: list[str], chunks: list[str]) -> np.array:\n",
    "    \n",
    "    embeddings_query = get_embeddings(model, query, model_string)\n",
    "    embeddings_chunks = get_embeddings(model, chunks, model_string)\n",
    "\n",
    "    print(embeddings_query.shape)\n",
    "    print(embeddings_chunks.shape)\n",
    "    \n",
    "    # embedding_query = embeddings[0]\n",
    "    # embedding_chunks = embeddings[1:]\n",
    "    \n",
    "    sim_list = []\n",
    "    for e_chunk in embeddings_chunks:\n",
    "        sim = cosine_similarity(embeddings_query, np.expand_dims(e_chunk, 0))\n",
    "        sim_list.append(sim)\n",
    "    sim_list = np.squeeze(sim_list)\n",
    "\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de4be1-f3e7-4166-9032-b2606de20725",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_string = \"WhereIsAI/UAE-Large-V1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc240cf4-8c9b-41c0-9717-d8549aa4351c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc435f7-7468-4cc7-999e-17d8239d6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"I want to find information about a high-ranking personnel in the army.\",\n",
    "           \"I want to find information about a high-ranking personnel in the army. This person is a female.\",\n",
    "           \"I want to find information about a high-ranking personnel in the army. This person is a male.\",\n",
    "           \"I want to find information about a high-ranking personnel in the army. This person's gender is not known.\"\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6e4aae-1fe0-4220-a29f-c599760f4e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"She commanded the largest naval fleet in the history of the nation.\",\n",
    "    \"The colonel devised a successful counterinsurgency strategy. She turned the tide of the conflict.\",\n",
    "    \"The officer was promoted to brigadier general after her decisive victory in the northern territories.\",\n",
    "    \"The general authored a highly regarded manual on modern warfare tactics. A brilliant lady!\",\n",
    "    \"She oversaw the construction of the countrys most advanced aircraft carrier.\",\n",
    "    \n",
    "    \"He commanded the largest naval fleet in the history of the nation.\",\n",
    "    \"The colonel devised a successful counterinsurgency strategy. He turned the tide of the conflict.\",\n",
    "    \"The officer was promoted to brigadier general after his decisive victory in the northern territories.\",\n",
    "    \"The general authored a highly regarded manual on modern warfare tactics. A brilliant sir!\",\n",
    "    \"He oversaw the construction of the countrys most advanced aircraft carrier.\",\n",
    "    \n",
    "    \"This person commanded the largest naval fleet in the history of the nation.\",\n",
    "    \"The colonel devised a successful counterinsurgency strategy. This person turned the tide of the conflict.\",\n",
    "    \"The officer was promoted to brigadier general after this person's decisive victory in the northern territories.\",\n",
    "    \"The general authored a highly regarded manual on modern warfare tactics. A brilliant person!\",\n",
    "    \"This person oversaw the construction of the countrys most advanced aircraft carrier.\",\n",
    "\n",
    "    \"A cat stretched lazily on the windowsill, basking in the warmth of the afternoon sun.\",\n",
    "    \"The train rattled along the tracks, carrying passengers through the misty countryside.\",\n",
    "    \"A musician played his guitar under the streetlight, his melodies echoing through the quiet night.\",\n",
    "    \"The chef chopped vegetables with precision, the sound of the knife rhythmic against the cutting board.\",\n",
    "    \"A young couple walked hand in hand along the beach, the waves gently lapping at their feet.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce380a-6360-4970-9421-5d358b34c23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fbf293-38e6-4c43-b621-658070dee2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_queries = get_embeddings(model, queries, model_string)\n",
    "embeddings_chunks = get_embeddings(model, sentences, model_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d576197-2e70-416e-a946-55ecd4b53c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = cosine_similarity(embeddings_queries, embeddings_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95379972-71a8-4b8a-b521-e454f8bd97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = similarity_matrix[:, :15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d8d67fc-a340-451c-a48c-c9ea62dc7ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 18))  # Adjusted size\n",
    "\n",
    "# Create the heatmap with horizontal color bar\n",
    "ax = sns.heatmap(similarity_matrix, cmap=\"viridis\", linewidths=0.5, linecolor=\"white\", square=True, cbar=True,\n",
    "                  cbar_kws={\"shrink\": 0.5, \"aspect\": 20, \"orientation\": \"horizontal\", \"location\":\"top\"},  # Set orientation to horizontal\n",
    "                  )\n",
    "\n",
    "# Access the color bar and set the font size\n",
    "colorbar = ax.collections[0].colorbar\n",
    "colorbar.ax.tick_params(labelsize=24)  # Set the font size of the colorbar labels\n",
    "colorbar.ax.xaxis.set_ticks_position('top')\n",
    "\n",
    "# Add custom text labels across the 5 ticks\n",
    "plt.text(2.5, 4.7, 'Female', ha='center', fontsize=28)  \n",
    "plt.text(7.5, 4.7, 'Male', ha='center', fontsize=28)    \n",
    "plt.text(12.5, 4.7, 'Neutral', ha='center', fontsize=28) \n",
    "\n",
    "plt.axvline(x=5, color='red', linestyle='--', linewidth=4) \n",
    "plt.axvline(x=10, color='red', linestyle='--', linewidth=4) \n",
    "\n",
    "custom_yticks = ['Q: neutral', 'Q: female', 'Q: male', 'Q: debiasing']\n",
    "ax.set_yticklabels(custom_yticks, rotation=0, fontsize=28) \n",
    "ax.set_xticklabels([f\"$C_{{{val}}}$\" for val in [1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5]], fontsize=23)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to minimize whitespace\n",
    "plt.savefig(f\"similarity_matrix.pdf\", format='pdf', dpi=300, bbox_inches='tight')  # Save without excess padding\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90687809-a9b7-43e5-893f-fc3e32640457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12267b5-abcd-45dd-9b44-1438c62a221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mock retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f190581e-64e6-40bc-8cd1-994e9097bb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"female\"] * 5 + [\"male\"] * 5 + [\"neutral\"] * 5 + [\"random\"] * 5\n",
    "symbols = [f\"C_{val}\" for val in [1, 2, 3, 4, 5]] * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8e63d4-c8da-4c0b-a64e-c4a02eef623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [f\"{labels[i]}_{symbols[i]}\" for i in range(len(labels))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abea8da-212c-4574-8cea-317e8aea2d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd6ca6-9962-4045-8d0c-6ee499b0a38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = compute_similarities(neutral_query, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b9e5fd-e39b-4022-82d8-541794a8732d",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [\"#DC143C\", \"#4682B4\", \"#2E8B57\", \"#D2691E\"]\n",
    "\n",
    "\n",
    "queries = [\"I want to find information about a high-ranking personnel in the army.\",\n",
    "           \"I want to find information about a high-ranking personnel in the army. This person is a female.\",\n",
    "           \"I want to find information about a high-ranking personnel in the army. This person's gender is not known.\"]\n",
    "\n",
    "# Create subplots: 1 row, 3 columns\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))  # 3 subplots in a row, width 15, height 5\n",
    "\n",
    "for idx, query in enumerate(queries):\n",
    "    sim_list = compute_similarities(query, sentences)\n",
    "    \n",
    "    ax = axes[idx]  # Select the subplot to plot on\n",
    "    ax.grid()\n",
    "    ax.plot(range(5), sim_list[:5], c=colors[0])\n",
    "    ax.plot(range(5), sim_list[5:10], c=colors[1])\n",
    "    ax.plot(range(5), sim_list[10:15], c=colors[2])\n",
    "    ax.plot(range(len(sim_list[15:])), sim_list[15:], c=colors[3])\n",
    "    \n",
    "    ax.set_ylabel(\"Cosine Similarity\", size=13)\n",
    "    ax.set_xticks(np.arange(5))\n",
    "    ax.tick_params(axis='y', labelsize=13)\n",
    "    ax.set_xticklabels([f\"$C_{{{val}}}$\" for val in [1, 2, 3, 4, 5]], fontsize=13)\n",
    "    ax.legend([\"Female\", \"Male\", \"Neutral\", \"Random\"], loc=\"upper left\", fontsize=12)\n",
    "    \n",
    "plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "plt.savefig(f\"similarity_plot_queries.pdf\", format='pdf', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ccafff-17cd-4881-9fda-f97d6264346f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels)[np.argsort(-sim_list)] # this is what we get for a regular retrieval using a neutral query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d7e5f-9146-4e96-8d25-f2fa5767726f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccf7643-10b7-483d-9dfd-b7c68366bf26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f08ee-cb35-424e-841a-591b1b87e707",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = ['''I want to find information about a high-ranking personnel in the army. \n",
    "                This person is a female.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd728a6a-3c59-44c7-a9da-bdd219df2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = compute_similarities(new_query, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54e4a18-b8bd-416f-8b2d-5d55b1267be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.grid()\n",
    "plt.plot(range(5), sim_list[:5])\n",
    "plt.plot(range(5), sim_list[5:10])\n",
    "plt.plot(range(5), sim_list[10:15])\n",
    "plt.plot(range(len(sim_list[15:])), sim_list[15:])\n",
    "plt.xticks(range(5))\n",
    "plt.legend([\"female\", \"male\", \"neutral\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a937c797-e784-467b-bd52-df362ca5ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10 # assume retrieve top 10\n",
    "\n",
    "# We will want to retrieve all these results when using the male query below\n",
    "np.array(labels)[np.argsort(-sim_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27d1861-413a-43ce-933a-01e4db57f0b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b238638-b067-46fc-bcaa-b82c7c4951d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3e4a80-028d-46cf-b9b1-15bb1bc5fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = ['''I want to find information about a high-ranking personnel in the army. \n",
    "                This person is a male.''']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf346a-1d5d-4c14-8c7e-8cf87b8360ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_list = compute_similarities(new_query, sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56137f4-9929-4b24-91a4-9a27b1c8e43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.grid()\n",
    "plt.plot(range(5), sim_list[:5])\n",
    "plt.plot(range(5), sim_list[5:10])\n",
    "plt.plot(range(5), sim_list[10:15])\n",
    "plt.xticks(range(5))\n",
    "plt.legend([\"female\", \"male\", \"neutral\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea6c8e4-8e8e-4ada-8591-31e851b472c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e847d0db-8a80-4d7f-9158-8795d8668e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From above, using the female query, we retreived : \n",
    "#   array(['female_C_4', 'female_C_3', 'female_C_2', 'neutral_C_4',\n",
    "#          'male_C_4', 'neutral_C_3', 'female_C_5', 'neutral_C_2',\n",
    "#          'female_C_1', 'male_C_3'], dtype='<U11')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc8a82a-0585-4567-9e38-f405d9146009",
   "metadata": {},
   "source": [
    "Now we do not use k anymore, but set the threshold at the lowest chunk from above. We see below that the lowest chunk is given \n",
    "by 'female_C_5'. Thus we retrieve all the chunks before this chunk, including 'female_C_5' itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680f8bc6-c981-4c64-8c27-1ba5cb1e875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(labels)[np.argsort(-sim_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af774c61-54c2-4237-8b36-e957f5346f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Therefore we retrieve:\n",
    "\n",
    "#       'neutral_C_4', 'male_C_4', 'neutral_C_2', 'neutral_C_3',\n",
    "#       'male_C_3', 'male_C_2', 'female_C_4', 'neutral_C_1', 'female_C_3',\n",
    "#       'female_C_2', 'neutral_C_5', 'male_C_1', 'male_C_5', 'female_C_1',\n",
    "#       'female_C_5',"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a030d9c-7765-4f14-b48e-c1080af16663",
   "metadata": {},
   "source": [
    "This can be seen as setting the number of retrieved chunks dynamically, sweeping across both gender and neutral.\n",
    "In this case, we correctly retrieve all the relevant chunks while leaving out the random ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af9ddc0-538a-4afa-8603-a85ef69b7f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6afd84-a5a3-41ab-844e-4b3488e9a72d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853d2232-6cb7-4322-8d56-1907e299e03f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
